{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMgC9hv43euQ"
      },
      "outputs": [],
      "source": [
        "# !pip install google-generativeai langchain langchain-google-genai tavily-python\n",
        "# !pip install faiss-cpu beautifulsoup4 pypdf\n",
        "# !pip install langchain-community\n",
        "# !pip -q install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q langtrace-python-sdk"
      ],
      "metadata": {
        "id": "KY1Ong9Q_6p2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import os"
      ],
      "metadata": {
        "id": "JbcrrlwZ3twa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langtrace_python_sdk import langtrace\n",
        "\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"TAVILY_API_KEY\")\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "langtrace.init(api_key = userdata.get('LANGTRACE_API_KEY'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YL5lM2514Djx",
        "outputId": "aa3b00e1-9661-48b6-ffbe-c006df71e98d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:opentelemetry.instrumentation.instrumentor:DependencyConflict: requested: \"groq >= 0.5.0\" but found: \"None\"\n",
            "ERROR:opentelemetry.instrumentation.instrumentor:DependencyConflict: requested: \"pinecone-client >= 3.1.0\" but found: \"None\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mInitializing Langtrace SDK..\u001b[39m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:opentelemetry.instrumentation.instrumentor:DependencyConflict: requested: \"llama-index >= 0.10.0\" but found: \"None\"\n",
            "ERROR:opentelemetry.instrumentation.instrumentor:DependencyConflict: requested: \"chromadb >= 0.4.23\" but found: \"None\"\n",
            "ERROR:opentelemetry.instrumentation.instrumentor:DependencyConflict: requested: \"qdrant-client >= 1.9.0\" but found: \"None\"\n",
            "ERROR:opentelemetry.instrumentation.instrumentor:DependencyConflict: requested: \"langgraph >= 0.0.39\" but found: \"None\"\n",
            "ERROR:opentelemetry.instrumentation.instrumentor:DependencyConflict: requested: \"anthropic >= 0.19.1\" but found: \"None\"\n",
            "ERROR:opentelemetry.instrumentation.instrumentor:DependencyConflict: requested: \"cohere >= 5.0.0\" but found: \"None\"\n",
            "ERROR:opentelemetry.instrumentation.instrumentor:DependencyConflict: requested: \"weaviate-client >= 4.6.1\" but found: \"None\"\n",
            "ERROR:opentelemetry.instrumentation.instrumentor:DependencyConflict: requested: \"ollama >= 0.2.0, < 1\" but found: \"None\"\n",
            "ERROR:opentelemetry.instrumentation.instrumentor:DependencyConflict: requested: \"dspy-ai >= 2.0.0\" but found: \"None\"\n",
            "ERROR:opentelemetry.instrumentation.instrumentor:DependencyConflict: requested: \"crewai >= 0.32.0\" but found: \"None\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34mExporting spans to Langtrace cloud..\u001b[39m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Data"
      ],
      "metadata": {
        "id": "B-Gvmx-65F0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(\"Transformers.pdf\")\n",
        "docs = loader.load_and_split()"
      ],
      "metadata": {
        "id": "Jrzz2p4O4r6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-I8ayTn5JHm",
        "outputId": "157cd78c-9228-41ec-854d-d0aaa632a14c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split into Chunks"
      ],
      "metadata": {
        "id": "VphayquS5oof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet langchain-text-splitters tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTCUD31v6Gxd",
        "outputId": "e6fef432-95be-42ef-924c-4d8672030e63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mExported 8 spans successfully.\u001b[39m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# always split on tokens, not characters\n",
        "# https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/split_by_token/"
      ],
      "metadata": {
        "id": "FsRivFJy6YQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    model_name=\"gpt-4\",\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=100)\n",
        "\n",
        "documents = text_splitter.split_documents(docs)\n",
        "\n",
        "documents[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgpY4pKe5K5S",
        "outputId": "eb90d334-6795-48c7-88d8-b5e075c75b0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'Transformers.pdf', 'page': 0}, page_content='Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started'),\n",
              " Document(metadata={'source': 'Transformers.pdf', 'page': 0}, page_content='in particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.'),\n",
              " Document(metadata={'source': 'Transformers.pdf', 'page': 1}, page_content='Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstatesht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [ 18] and conditional\\ncomputation [ 26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,16]. In all but a few cases [ 22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [ 15] and ConvS2S [ 8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions'),\n",
              " Document(metadata={'source': 'Transformers.pdf', 'page': 1}, page_content='it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,29].\\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n)to a sequence\\nof continuous representations z= (z1,...,z n). Given z, the decoder then generates an output\\nsequence (y1,...,y m)of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully'),\n",
              " Document(metadata={'source': 'Transformers.pdf', 'page': 1}, page_content='[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of documents: {len(documents)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nx_h27wB54ly",
        "outputId": "ea0d8f91-1394-497c-9ede-41de1eba991f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents: 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(documents[5].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hcfi0-QN6q85",
        "outputId": "80b8eeb3-a8e5-417e-f1c2-be14ba82a31a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Figure 1: The Transformer - model architecture.\n",
            "wise fully connected feed-forward network. We employ a residual connection [ 10] around each of\n",
            "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
            "LayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer\n",
            "itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
            "layers, produce outputs of dimension dmodel = 512 .\n",
            "Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\n",
            "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\n",
            "attention over the output of the encoder stack. Similar to the encoder, we employ residual connections\n",
            "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
            "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
            "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
            "predictions for position ican depend only on the known outputs at positions less than i.\n",
            "3.2 Attention\n",
            "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
            "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
            "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
            "query with the corresponding key.\n",
            "3.2.1 Scaled Dot-Product Attention\n",
            "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
            "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings Model"
      ],
      "metadata": {
        "id": "WD8zAGEc65Cv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
      ],
      "metadata": {
        "id": "igod10Yl6sZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXLs9gMs66tV",
        "outputId": "8c888ceb-a8aa-46ee-8d95-22cfec713be3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GoogleGenerativeAIEmbeddings(client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7ff20020c790>, model='models/embedding-001', task_type=None, google_api_key=None, credentials=None, client_options=None, transport=None, request_options=None)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.embed_query(documents[0].page_content)\n",
        "print(f\"lenghth of embedding: {len(embeddings.embed_query(documents[0].page_content))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjbxLYTl7QDb",
        "outputId": "84030153-7c92-4974-ad48-26de48444e01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lenghth of embedding: 768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector = FAISS.from_documents(documents, embeddings)\n"
      ],
      "metadata": {
        "id": "S3BTz1vt7ZdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector.as_retriever()\n"
      ],
      "metadata": {
        "id": "0-g6pdo47h0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.invoke(\"What is the Transformer Architecture?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5SxKoOa7lnp",
        "outputId": "09f97586-335e-4e39-c7c8-32a8e5e73ab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'Transformers.pdf', 'page': 1}, page_content='[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2'),\n",
              " Document(metadata={'source': 'Transformers.pdf', 'page': 1}, page_content='it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,29].\\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n)to a sequence\\nof continuous representations z= (z1,...,z n). Given z, the decoder then generates an output\\nsequence (y1,...,y m)of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully'),\n",
              " Document(metadata={'source': 'Transformers.pdf', 'page': 2}, page_content='Figure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [ 10] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .\\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3'),\n",
              " Document(metadata={'source': 'Transformers.pdf', 'page': 1}, page_content='Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstatesht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [ 18] and conditional\\ncomputation [ 26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,16]. In all but a few cases [ 22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [ 15] and ConvS2S [ 8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions')]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZrK0dZYV7q3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create a retrieval tool"
      ],
      "metadata": {
        "id": "mIy-i93d8DnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create tool for retrieving documents\n",
        "from langchain.tools.retriever import create_retriever_tool"
      ],
      "metadata": {
        "id": "7yO7mvDe8AH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_tool = create_retriever_tool(\n",
        "    retriever=retriever,\n",
        "    name=\"retriever_tool\",\n",
        "    description=\"\"\"For any information related to transformers architecture, use this tool.\n",
        "    Useful for retrieving information related to the Attention is all you need paper.\"\"\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "SSNYj5Dh8FyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create a search tool"
      ],
      "metadata": {
        "id": "WFglhlGV8l7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_tool = TavilySearchResults()"
      ],
      "metadata": {
        "id": "kN11Z7db8gaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_tool"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6foS_US8otO",
        "outputId": "b4ebf6cd-1778-4a35-ac90-ba01aadc12df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TavilySearchResults()"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search_tool.dict()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5E8e7r98tJK",
        "outputId": "89ceb731-1296-4c9b-de7f-7ccc903a4378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'tavily_search_results_json',\n",
              " 'description': 'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.',\n",
              " 'args_schema': langchain_community.tools.tavily_search.tool.TavilyInput,\n",
              " 'return_direct': False,\n",
              " 'verbose': False,\n",
              " 'tags': None,\n",
              " 'metadata': None,\n",
              " 'handle_tool_error': False,\n",
              " 'handle_validation_error': False,\n",
              " 'response_format': 'content_and_artifact',\n",
              " 'max_results': 5,\n",
              " 'search_depth': 'advanced',\n",
              " 'include_domains': [],\n",
              " 'exclude_domains': [],\n",
              " 'include_answer': False,\n",
              " 'include_raw_content': False,\n",
              " 'include_images': False,\n",
              " 'api_wrapper': {'tavily_api_key': SecretStr('**********')}}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create list of tools"
      ],
      "metadata": {
        "id": "O9uHEeNU8_Sh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [search_tool, retriever_tool]\n"
      ],
      "metadata": {
        "id": "BF8UJRkS8vr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LLM - Gemini Pro Experimental"
      ],
      "metadata": {
        "id": "F7bO4S9z9FWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "llm = GoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "# llm = GoogleGenerativeAI(model=\"gemini-1.5-pro-exp-0801\", google_api_key=os.environ[\"GOOGLE_API_KEY\"])"
      ],
      "metadata": {
        "id": "U8orjTOU9Bcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtgwI4qC9niQ",
        "outputId": "f91935bc-3a44-43d6-986c-4f9dd7bca96f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GoogleGenerativeAI(model='gemini-1.5-pro', google_api_key=SecretStr('**********'), client=genai.GenerativeModel(\n",
              "    model_name='models/gemini-1.5-pro',\n",
              "    generation_config={},\n",
              "    safety_settings={},\n",
              "    tools=None,\n",
              "    system_instruction=None,\n",
              "    cached_content=None\n",
              "))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create The AGENT"
      ],
      "metadata": {
        "id": "UPOz7KjW9tUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import create_react_agent, AgentExecutor\n",
        "from langchain import hub"
      ],
      "metadata": {
        "id": "J70BJKze9oVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchainhub"
      ],
      "metadata": {
        "id": "bxKA9TB_90yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = hub.pull(\"hwchase17/react\")"
      ],
      "metadata": {
        "id": "EhozcnRB9wGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt.template)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQtJQxOu9ywH",
        "outputId": "450c65ca-042e-4442-9706-004a4962210a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "{tools}\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [{tool_names}]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "\n",
            "Question: {input}\n",
            "Thought:{agent_scratchpad}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent = create_react_agent(llm=llm, tools=tools, prompt=prompt)\n"
      ],
      "metadata": {
        "id": "23YyLQ0L99oY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFwexlNI-Ih2",
        "outputId": "5167f232-eb68-4906-d31c-04265faf50f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RunnableAssign(mapper={\n",
              "  agent_scratchpad: RunnableLambda(lambda x: format_log_to_str(x['intermediate_steps']))\n",
              "})\n",
              "| PromptTemplate(input_variables=['agent_scratchpad', 'input'], partial_variables={'tools': \"tavily_search_results_json - A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.\\nretriever_tool(query: 'str', *, retriever: 'BaseRetriever' = VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7ff2000c1ff0>), document_prompt: 'BasePromptTemplate' = PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator: 'str' = '\\\\n\\\\n', callbacks: 'Callbacks' = None) -> 'str' - For any information related to transformers architecture, use this tool. \\n    Useful for retrieving information related to the Attention is all you need paper.\", 'tool_names': 'tavily_search_results_json, retriever_tool'}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'react', 'lc_hub_commit_hash': 'd15fe3c426f1c4b3f37c9198853e4a86e20c425ca7f4752ec0c9b0e97ca7ea4d'}, template='Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}')\n",
              "| RunnableBinding(bound=GoogleGenerativeAI(model='gemini-1.5-pro', google_api_key=SecretStr('**********'), client=genai.GenerativeModel(\n",
              "      model_name='models/gemini-1.5-pro',\n",
              "      generation_config={},\n",
              "      safety_settings={},\n",
              "      tools=None,\n",
              "      system_instruction=None,\n",
              "      cached_content=None\n",
              "  )), kwargs={'stop': ['\\nObservation']})\n",
              "| ReActSingleInputOutputParser()"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YseKMa5d-P61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create the Agent Executor"
      ],
      "metadata": {
        "id": "k_wrflVh-Qpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executer = AgentExecutor(agent=agent, tools=tools, verbose=True)\n"
      ],
      "metadata": {
        "id": "isK-ZjFE-JPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8X1eI0P-Tfc",
        "outputId": "96caac48-0fcd-4054-f0f1-2e71ade8554f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AgentExecutor(verbose=True, agent=RunnableAgent(runnable=RunnableAssign(mapper={\n",
              "  agent_scratchpad: RunnableLambda(lambda x: format_log_to_str(x['intermediate_steps']))\n",
              "})\n",
              "| PromptTemplate(input_variables=['agent_scratchpad', 'input'], partial_variables={'tools': \"tavily_search_results_json - A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.\\nretriever_tool(query: 'str', *, retriever: 'BaseRetriever' = VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7ff2000c1ff0>), document_prompt: 'BasePromptTemplate' = PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator: 'str' = '\\\\n\\\\n', callbacks: 'Callbacks' = None) -> 'str' - For any information related to transformers architecture, use this tool. \\n    Useful for retrieving information related to the Attention is all you need paper.\", 'tool_names': 'tavily_search_results_json, retriever_tool'}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'react', 'lc_hub_commit_hash': 'd15fe3c426f1c4b3f37c9198853e4a86e20c425ca7f4752ec0c9b0e97ca7ea4d'}, template='Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}')\n",
              "| RunnableBinding(bound=GoogleGenerativeAI(model='gemini-1.5-pro', google_api_key=SecretStr('**********'), client=genai.GenerativeModel(\n",
              "      model_name='models/gemini-1.5-pro',\n",
              "      generation_config={},\n",
              "      safety_settings={},\n",
              "      tools=None,\n",
              "      system_instruction=None,\n",
              "      cached_content=None\n",
              "  )), kwargs={'stop': ['\\nObservation']})\n",
              "| ReActSingleInputOutputParser(), input_keys_arg=[], return_keys_arg=[], stream_runnable=True), tools=[TavilySearchResults(), Tool(name='retriever_tool', description='For any information related to transformers architecture, use this tool. \\n    Useful for retrieving information related to the Attention is all you need paper.', args_schema=<class 'langchain_core.tools.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x7ff2288f08b0>, retriever=VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7ff2000c1ff0>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'), coroutine=functools.partial(<function _aget_relevant_documents at 0x7ff2288f0940>, retriever=VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7ff2000c1ff0>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'))])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executer.invoke({\"input\":\"What is the current weather at LA?\"})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kd3C-hRB-Upa",
        "outputId": "d92bfdee-f715-45dd-c1ef-a998c906502b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mQuestion: What is the current weather at LA?\n",
            "Thought: I need to get the current weather information, so I should use the search engine.\n",
            "Action: tavily_search_results_json\n",
            "Action Input: current weather in Los Angeles\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.weatherapi.com/', 'content': \"{'location': {'name': 'Los Angeles', 'region': 'California', 'country': 'United States of America', 'lat': 34.05, 'lon': -118.24, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1723010122, 'localtime': '2024-08-06 22:55'}, 'current': {'last_updated_epoch': 1723009500, 'last_updated': '2024-08-06 22:45', 'temp_c': 24.9, 'temp_f': 76.7, 'is_day': 0, 'condition': {'text': 'Clear', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 6.9, 'wind_kph': 11.2, 'wind_degree': 152, 'wind_dir': 'SSE', 'pressure_mb': 1014.0, 'pressure_in': 29.96, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 56, 'cloud': 0, 'feelslike_c': 26.0, 'feelslike_f': 78.7, 'windchill_c': 24.8, 'windchill_f': 76.7, 'heatindex_c': 26.0, 'heatindex_f': 78.7, 'dewpoint_c': 15.0, 'dewpoint_f': 59.0, 'vis_km': 10.0, 'vis_miles': 6.0, 'uv': 1.0, 'gust_mph': 8.5, 'gust_kph': 13.7}}\"}, {'url': 'https://www.ventusky.com/32.87;-117.24', 'content': 'Location ☀ Weather forecast for 10 days, information from meteorological stations, webcams, sunrise and sunset, wind and precipitation maps for this place ... America/Los_Angeles (UTC-7) / Current time: 07:20 2024/08/03 . Current Weather ; Forecast ; Sun and Moon ... (06:53 2024/08/03) Weather for the next 24 hours . 08:00 09:00 10:00 11:00 ...'}, {'url': 'https://forecast.weather.gov/MapClick.php?lat=33.9436&lon=-118.408', 'content': 'Los Angeles, Los Angeles International Airport (KLAX) Lat: 33.93806°NLon: 118.38889°WElev: ... 08 am PDT Aug 6, 2024. Forecast Valid: 8am PDT Aug 6, 2024-6pm PDT Aug 12, 2024 . Forecast Discussion . ... Severe Weather ; Current Outlook Maps ; Drought ; Fire Weather ; Fronts/Precipitation Maps ; Current Graphical Forecast Maps ;'}, {'url': 'https://www.meteoprog.com/weather/Losangeles/month/august/', 'content': 'Los Angeles (United States) weather in August 2024 ☀️ Accurate weather forecast for Los Angeles in August ⛅ Detailed forecast By month Current temperature \"near me\" Weather news ⊳ Widget of weather ⊳ Water temperature | METEOPROG'}, {'url': 'https://forecast.weather.gov/zipcity.php?inputstring=lax', 'content': 'Current conditions at Los Angeles, Los Angeles International Airport (KLAX) Lat: 33.93806°NLon: ... 2024 . Forecast Discussion . Additional Resources. Radar & Satellite Image. Hourly Weather Forecast. National Digital Forecast Database. ... National Weather Service; Los Angeles, CA; 520 North Elevar Street; Oxnard, CA 93030; Comments ...'}]\u001b[0m\u001b[32mExported 6 spans successfully.\u001b[39m\n",
            "\u001b[32;1m\u001b[1;3mThought: The first result from weatherapi.com has the information I need.\n",
            "Action: tavily_search_results_json\n",
            "Action Input: current weather in Los Angeles\u001b[0m\u001b[32mExported 6 spans successfully.\u001b[39m\n",
            "\u001b[36;1m\u001b[1;3m[{'url': 'https://www.weatherapi.com/', 'content': \"{'location': {'name': 'Los Angeles', 'region': 'California', 'country': 'United States of America', 'lat': 34.05, 'lon': -118.24, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1723010122, 'localtime': '2024-08-06 22:55'}, 'current': {'last_updated_epoch': 1723009500, 'last_updated': '2024-08-06 22:45', 'temp_c': 24.9, 'temp_f': 76.7, 'is_day': 0, 'condition': {'text': 'Clear', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 6.9, 'wind_kph': 11.2, 'wind_degree': 152, 'wind_dir': 'SSE', 'pressure_mb': 1014.0, 'pressure_in': 29.96, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 56, 'cloud': 0, 'feelslike_c': 26.0, 'feelslike_f': 78.7, 'windchill_c': 24.8, 'windchill_f': 76.7, 'heatindex_c': 26.0, 'heatindex_f': 78.7, 'dewpoint_c': 15.0, 'dewpoint_f': 59.0, 'vis_km': 10.0, 'vis_miles': 6.0, 'uv': 1.0, 'gust_mph': 8.5, 'gust_kph': 13.7}}\"}, {'url': 'https://forecast.weather.gov/MapClick.php?lat=33.9436&lon=-118.408', 'content': 'Los Angeles, Los Angeles International Airport (KLAX) Lat: 33.93806°NLon: 118.38889°WElev: ... 08 am PDT Aug 6, 2024. Forecast Valid: 8am PDT Aug 6, 2024-6pm PDT Aug 12, 2024 . Forecast Discussion . ... Severe Weather ; Current Outlook Maps ; Drought ; Fire Weather ; Fronts/Precipitation Maps ; Current Graphical Forecast Maps ;'}, {'url': 'https://forecast.weather.gov/zipcity.php?inputstring=lax', 'content': 'Current conditions at Los Angeles, Los Angeles International Airport (KLAX) Lat: 33.93806°NLon: ... 2024-6pm PDT Aug 7, 2024 . Forecast Discussion . Additional Resources. ... National Weather Service; Los Angeles, CA; 520 North Elevar Street; Oxnard, CA 93030; Comments? Questions? Please Contact Us.'}, {'url': 'https://forecast.weather.gov/MapClick.php?site=LOX&textField1=34.0522&textField2=-118.243&e=0', 'content': 'Los Angeles CA 34.05°N 118.25°W (Elev. 377 ft) Last Update: 9:48 pm PDT Aug 5, 2024. Forecast Valid: 3am PDT Aug 6, 2024-6pm PDT Aug 12, 2024 . Forecast Discussion . Additional Resources. Radar & Satellite Image. Hourly Weather Forecast. ... Severe Weather ; Current Outlook Maps ; Drought ; Fire Weather ; Fronts/Precipitation Maps ; Current ...'}, {'url': 'https://world-weather.info/forecast/usa/los_angeles/august-2024/', 'content': 'Hourly Week 10 days 14 days 30 days Year. Detailed ⚡ Los Angeles Weather Forecast for August 2024 - day/night 🌡️ temperatures, precipitations - World-Weather.info.'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: I can get the current temperature from the 'content' field of the first search result.\n",
            "Action: tavily_search_results_json\n",
            "Action Input: current weather in Los Angeles\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.weatherapi.com/', 'content': \"{'location': {'name': 'Los Angeles', 'region': 'California', 'country': 'United States of America', 'lat': 34.05, 'lon': -118.24, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1723010122, 'localtime': '2024-08-06 22:55'}, 'current': {'last_updated_epoch': 1723009500, 'last_updated': '2024-08-06 22:45', 'temp_c': 24.9, 'temp_f': 76.7, 'is_day': 0, 'condition': {'text': 'Clear', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 6.9, 'wind_kph': 11.2, 'wind_degree': 152, 'wind_dir': 'SSE', 'pressure_mb': 1014.0, 'pressure_in': 29.96, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 56, 'cloud': 0, 'feelslike_c': 26.0, 'feelslike_f': 78.7, 'windchill_c': 24.8, 'windchill_f': 76.7, 'heatindex_c': 26.0, 'heatindex_f': 78.7, 'dewpoint_c': 15.0, 'dewpoint_f': 59.0, 'vis_km': 10.0, 'vis_miles': 6.0, 'uv': 1.0, 'gust_mph': 8.5, 'gust_kph': 13.7}}\"}, {'url': 'https://forecast.weather.gov/MapClick.php?lat=33.9436&lon=-118.408', 'content': 'Los Angeles, Los Angeles International Airport (KLAX) Lat: 33.93806°NLon: 118.38889°WElev: ... 08 am PDT Aug 6, 2024. Forecast Valid: 8am PDT Aug 6, 2024-6pm PDT Aug 12, 2024 . Forecast Discussion . ... Severe Weather ; Current Outlook Maps ; Drought ; Fire Weather ; Fronts/Precipitation Maps ; Current Graphical Forecast Maps ;'}, {'url': 'https://forecast.weather.gov/zipcity.php?inputstring=Los Angeles,CA', 'content': 'Los Angeles CA 34.05°N 118.25°W (Elev. 377 ft) Last Update: 9:48 pm PDT Aug 5, 2024. Forecast Valid: 10pm PDT Aug 5, 2024-6pm PDT Aug 12, 2024 . Forecast Discussion . Additional Resources. Radar & Satellite Image. Hourly Weather Forecast. ... Severe Weather ; Current Outlook Maps ; Drought ; Fire Weather ; Fronts/Precipitation Maps ; Current ...'}, {'url': 'https://www.wunderground.com/weather/us/ca/los-angeles', 'content': 'Los Angeles Weather Forecasts. Weather Underground provides local & long-range weather forecasts, weatherreports, maps & tropical weather conditions for the Los Angeles area. ... 2024 (GMT -7 ...'}, {'url': 'https://www.weather.gov/index.php/lox/', 'content': 'Last Map Update: Sat, Aug 3, 2024 at 7:10:22 pm PDT Watches, Warnings & Advisories. Zoom Out. Excessive Heat Warning. Heat Advisory ... (Selected product opens in current window) Hazards. Observations. Marine Weather. Fire Weather ... National Weather Service Los Angeles, CA 520 North Elevar Street Oxnard, CA 93030 805-988-6610 Comments? ...'}]\u001b[0m\u001b[32mExported 6 spans successfully.\u001b[39m\n",
            "\u001b[32;1m\u001b[1;3mThought: I can get the current temperature from the 'content' field of the first search result. I will need to parse the JSON formatted string.\n",
            "Thought: I now know the final answer\n",
            "Final Answer: The current weather in Los Angeles is 24.9 degrees Celsius or 76.7 degrees Fahrenheit and Clear. \n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is the current weather at LA?',\n",
              " 'output': 'The current weather in Los Angeles is 24.9 degrees Celsius or 76.7 degrees Fahrenheit and Clear.'}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executer.invoke({\"input\":\"Can you give me a list of gold medals per country in the current Olympics? \"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSM6gXqo-eLr",
        "outputId": "543c1abb-3434-44e1-8d75-b168d8ad3db8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mQuestion: Can you give me a list of gold medals per country in the current Olympics? \n",
            "Thought: I need to get the latest information about the Olympics to answer this question. \n",
            "Action: tavily_search_results_json\n",
            "Action Input: olympic medal count 2024 \u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.usatoday.com/story/sports/olympics/2024/08/05/olympic-medals-today-medal-count-paris-games-aug-5/74668390007/', 'content': \"What is the medal count at the 2024 Paris Olympics? Here's the overall medal count on Monday. The U.S. leads the way with 75 total medals, and China leads with the most golds at 21.\"}, {'url': 'https://www.nytimes.com/interactive/2024/world/olympics-results-medals.html', 'content': 'The latest medal count and results for the Paris Olympics. ... Olympics Results. July 27, 2024. Leer en español. Share full article. Medal Count.'}, {'url': 'https://apnews.com/article/olympics-2024-medal-winners-today-b9522fd1223ae6599569ffe1ee48cc62', 'content': \"PARIS (AP) — The 2024 Olympics are underway. The United States led the medal standings after the 10th day of competition with China second and host France third, but more winners will join the list every day from July 27-Aug. 11. See which countries lead the medal count and the highlights in today's schedule.Below is a list of all the medal winners, day by day.\"}, {'url': 'https://olympics.com/en/paris-2024/medals?displayAsWebViewdark=true', 'content': 'Official medal standings for the Paris 2024 Olympics (Jul 26-Aug 11, 2024). Find out which athletes are bringing home medals and breaking records.'}, {'url': 'https://www.cbssports.com/olympics/news/2024-olympics-medal-count-tracker-for-how-many-gold-silver-bronze-medals-usa-nations-are-winning-in-paris/', 'content': 'Another day meant more medals for Team USA. The tenth day of the 2024 Paris Olympics began with a silver medal in the mixed triathlon relay team final, as the United States finished behind Germany ...'}]\u001b[0m\u001b[32mExported 6 spans successfully.\u001b[39m\n",
            "\u001b[32;1m\u001b[1;3mThought: I can try to get a medal table from one of these websites, which will list gold medals per country\n",
            "Action: tavily_search_results_json\n",
            "Action Input: 2024 olympics medal table\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://olympics.com/en/paris-2024/medals', 'content': \"Official medal standings for the Paris 2024 Olympics (Jul 26-Aug 11, 2024). Find out which athletes are bringing home medals and breaking records. ... Medal Table. Medal Table Medallists # NOCs. G. S. B. USA United States of America. 21 30 28 79. CHN People's Republic of China. 21 18 14 53. FRA France. 13 16 19 48. AUS Australia. 13 12 8 33 ...\"}, {'url': 'https://www.bbc.com/sport/olympics/paris-2024/medals', 'content': 'Find out the medal table from the Paris Olympics 2024, including overall medals, gold medals, silver medals and bronze medals for Team GB and other countries.'}, {'url': 'https://www.telegraph.co.uk/olympics/paris-2024-medal-table-results/', 'content': 'Latest medal table and results for the Paris Olympics 2024. ... Day of the Olympics Total number of medals won Loading data. Team GB vs other countries'}, {'url': 'https://olympics.com/en/paris-2024/medals/medallists', 'content': 'Official list of medallists at the Paris 2024 Olympics (Jul 26-Aug 11, 2024). Find out which athletes are bringing home medals and breaking records.'}, {'url': 'https://www.bbc.co.uk/sport/olympics/paris-2024/results', 'content': 'Find out the medal results from the Paris 2024 Olympics for each country, sport and event including athletics, swimming, football, cycling and more.'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: The first result from this search gives a list of gold medals per country\n",
            "Final Answer: According to the official medal standings for the Paris 2024 Olympics as of August 5, 2024, the USA and China are tied for the lead with 21 gold medals each. \n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Can you give me a list of gold medals per country in the current Olympics? ',\n",
              " 'output': 'According to the official medal standings for the Paris 2024 Olympics as of August 5, 2024, the USA and China are tied for the lead with 21 gold medals each.'}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executer.invoke({\"input\":\"Can you explain the concept of Attention in transformers?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kf_527Dt-vXe",
        "outputId": "bd966248-29fd-4674-bfc2-7b66136a59b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: The question asks for an explanation of \"Attention\" in the context of transformers. This is a concept related to the \"Attention is all you need\" paper, so I should use the `retriever_tool`. \n",
            "\n",
            "Action: retriever_tool\n",
            "Action Input: 'What is the concept of Attention in transformers, as described in the \"Attention is all you need\" paper?'\u001b[0m\u001b[33;1m\u001b[1;3mit more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\n",
            "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
            "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
            "described in section 3.2.\n",
            "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
            "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
            "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
            "textual entailment and learning task-independent sentence representations [4, 22, 23, 19].\n",
            "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
            "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
            "language modeling tasks [28].\n",
            "To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\n",
            "entirely on self-attention to compute representations of its input and output without using sequence-\n",
            "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
            "self-attention and discuss its advantages over models such as [14, 15] and [8].\n",
            "3 Model Architecture\n",
            "Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,29].\n",
            "Here, the encoder maps an input sequence of symbol representations (x1,...,x n)to a sequence\n",
            "of continuous representations z= (z1,...,z n). Given z, the decoder then generates an output\n",
            "sequence (y1,...,y m)of symbols one element at a time. At each step the model is auto-regressive\n",
            "[9], consuming the previously generated symbols as additional input when generating the next.\n",
            "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
            "\n",
            "Figure 1: The Transformer - model architecture.\n",
            "wise fully connected feed-forward network. We employ a residual connection [ 10] around each of\n",
            "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
            "LayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer\n",
            "itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
            "layers, produce outputs of dimension dmodel = 512 .\n",
            "Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\n",
            "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\n",
            "attention over the output of the encoder stack. Similar to the encoder, we employ residual connections\n",
            "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
            "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
            "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
            "predictions for position ican depend only on the known outputs at positions less than i.\n",
            "3.2 Attention\n",
            "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
            "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
            "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
            "query with the corresponding key.\n",
            "3.2.1 Scaled Dot-Product Attention\n",
            "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
            "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
            "3\n",
            "\n",
            "MultiHead( Q,K,V ) = Concat(head 1,...,head h)WO\n",
            "where head i= Attention( QWQ\n",
            "i,KWK\n",
            "i,VWV\n",
            "i)\n",
            "Where the projections are parameter matrices WQ\n",
            "i∈Rdmodel×dk,WK\n",
            "i∈Rdmodel×dk,WV\n",
            "i∈Rdmodel×dv\n",
            "andWO∈Rhdv×dmodel.\n",
            "In this work we employ h= 8 parallel attention layers, or heads. For each of these we use\n",
            "dk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\n",
            "is similar to that of single-head attention with full dimensionality.\n",
            "3.2.3 Applications of Attention in our Model\n",
            "The Transformer uses multi-head attention in three different ways:\n",
            "•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
            "and the memory keys and values come from the output of the encoder. This allows every\n",
            "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
            "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
            "[31, 2, 8].\n",
            "•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
            "and queries come from the same place, in this case, the output of the previous layer in the\n",
            "encoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
            "encoder.\n",
            "•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
            "all positions in the decoder up to and including that position. We need to prevent leftward\n",
            "information ﬂow in the decoder to preserve the auto-regressive property. We implement this\n",
            "inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\n",
            "of the softmax which correspond to illegal connections. See Figure 2.\n",
            "3.3 Position-wise Feed-Forward Networks\n",
            "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
            "connected feed-forward network, which is applied to each position separately and identically. This\n",
            "consists of two linear transformations with a ReLU activation in between.\n",
            "\n",
            "Attention Is All You Need\n",
            "Ashish Vaswani∗\n",
            "Google Brain\n",
            "avaswani@google.comNoam Shazeer∗\n",
            "Google Brain\n",
            "noam@google.comNiki Parmar∗\n",
            "Google Research\n",
            "nikip@google.comJakob Uszkoreit∗\n",
            "Google Research\n",
            "usz@google.com\n",
            "Llion Jones∗\n",
            "Google Research\n",
            "llion@google.comAidan N. Gomez∗†\n",
            "University of Toronto\n",
            "aidan@cs.toronto.eduŁukasz Kaiser∗\n",
            "Google Brain\n",
            "lukaszkaiser@google.com\n",
            "Illia Polosukhin∗‡\n",
            "illia.polosukhin@gmail.com\n",
            "Abstract\n",
            "The dominant sequence transduction models are based on complex recurrent or\n",
            "convolutional neural networks that include an encoder and a decoder. The best\n",
            "performing models also connect the encoder and decoder through an attention\n",
            "mechanism. We propose a new simple network architecture, the Transformer,\n",
            "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
            "entirely. Experiments on two machine translation tasks show these models to\n",
            "be superior in quality while being more parallelizable and requiring signiﬁcantly\n",
            "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
            "to-German translation task, improving over the existing best results, including\n",
            "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
            "our model establishes a new single-model state-of-the-art BLEU score of 41.0 after\n",
            "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
            "best models from the literature.\n",
            "1 Introduction\n",
            "Recurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\n",
            "in particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\n",
            "transduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\n",
            "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
            "architectures [31, 21, 13].\n",
            "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\u001b[0m\u001b[32mExported 12 spans successfully.\u001b[39m\n",
            "\u001b[32;1m\u001b[1;3mThought: The provided text describes the overall architecture and different types of attention in transformers, but it is lengthy and doesn't provide a concise explanation of the core concept. I need to refine my query to get a more focused answer. \n",
            "\n",
            "Action: retriever_tool\n",
            "Action Input: 'Explain the concept of \"Scaled Dot-Product Attention\" in transformers as simply as possible.'\u001b[0m\u001b[33;1m\u001b[1;3mFigure 1: The Transformer - model architecture.\n",
            "wise fully connected feed-forward network. We employ a residual connection [ 10] around each of\n",
            "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
            "LayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer\n",
            "itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
            "layers, produce outputs of dimension dmodel = 512 .\n",
            "Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\n",
            "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\n",
            "attention over the output of the encoder stack. Similar to the encoder, we employ residual connections\n",
            "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
            "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
            "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
            "predictions for position ican depend only on the known outputs at positions less than i.\n",
            "3.2 Attention\n",
            "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
            "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
            "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
            "query with the corresponding key.\n",
            "3.2.1 Scaled Dot-Product Attention\n",
            "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
            "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
            "3\n",
            "\n",
            "MultiHead( Q,K,V ) = Concat(head 1,...,head h)WO\n",
            "where head i= Attention( QWQ\n",
            "i,KWK\n",
            "i,VWV\n",
            "i)\n",
            "Where the projections are parameter matrices WQ\n",
            "i∈Rdmodel×dk,WK\n",
            "i∈Rdmodel×dk,WV\n",
            "i∈Rdmodel×dv\n",
            "andWO∈Rhdv×dmodel.\n",
            "In this work we employ h= 8 parallel attention layers, or heads. For each of these we use\n",
            "dk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\n",
            "is similar to that of single-head attention with full dimensionality.\n",
            "3.2.3 Applications of Attention in our Model\n",
            "The Transformer uses multi-head attention in three different ways:\n",
            "•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
            "and the memory keys and values come from the output of the encoder. This allows every\n",
            "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
            "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
            "[31, 2, 8].\n",
            "•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
            "and queries come from the same place, in this case, the output of the previous layer in the\n",
            "encoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
            "encoder.\n",
            "•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
            "all positions in the decoder up to and including that position. We need to prevent leftward\n",
            "information ﬂow in the decoder to preserve the auto-regressive property. We implement this\n",
            "inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\n",
            "of the softmax which correspond to illegal connections. See Figure 2.\n",
            "3.3 Position-wise Feed-Forward Networks\n",
            "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
            "connected feed-forward network, which is applied to each position separately and identically. This\n",
            "consists of two linear transformations with a ReLU activation in between.\n",
            "\n",
            "it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\n",
            "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
            "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
            "described in section 3.2.\n",
            "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
            "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
            "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
            "textual entailment and learning task-independent sentence representations [4, 22, 23, 19].\n",
            "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
            "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
            "language modeling tasks [28].\n",
            "To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\n",
            "entirely on self-attention to compute representations of its input and output without using sequence-\n",
            "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
            "self-attention and discuss its advantages over models such as [14, 15] and [8].\n",
            "3 Model Architecture\n",
            "Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,29].\n",
            "Here, the encoder maps an input sequence of symbol representations (x1,...,x n)to a sequence\n",
            "of continuous representations z= (z1,...,z n). Given z, the decoder then generates an output\n",
            "sequence (y1,...,y m)of symbols one element at a time. At each step the model is auto-regressive\n",
            "[9], consuming the previously generated symbols as additional input when generating the next.\n",
            "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
            "\n",
            "Scaled Dot-Product Attention\n",
            " Multi-Head Attention\n",
            "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
            "attention layers running in parallel.\n",
            "query with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\n",
            "values.\n",
            "In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
            "into a matrix Q. The keys and values are also packed together into matrices KandV. We compute\n",
            "the matrix of outputs as:\n",
            "Attention(Q,K,V ) = softmax(QKT\n",
            "√dk)V (1)\n",
            "The two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\n",
            "plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
            "of1√dk. Additive attention computes the compatibility function using a feed-forward network with\n",
            "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\n",
            "much faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\n",
            "matrix multiplication code.\n",
            "While for small values of dkthe two mechanisms perform similarly, additive attention outperforms\n",
            "dot product attention without scaling for larger values of dk[3]. We suspect that for large values of\n",
            "dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
            "extremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\n",
            "3.2.2 Multi-Head Attention\n",
            "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
            "we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\n",
            "linear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\n",
            "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
            "output values. These are concatenated and once again projected, resulting in the ﬁnal values, as\n",
            "depicted in Figure 2.\n",
            "Multi-head attention allows the model to jointly attend to information from different representation\n",
            "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
            "4To illustrate why the dot products get large, assume that the components of qandkare independent random\u001b[0m\u001b[32;1m\u001b[1;3mThought: The retriever tool is still not providing a concise explanation of \"Scaled Dot-Product Attention\". I will try rephrasing the query to focus on the core idea of calculating attention weights. \n",
            "\n",
            "Action: retriever_tool\n",
            "Action Input: 'How are attention weights calculated in \"Scaled Dot-Product Attention\" in transformers?' \u001b[0m\u001b[33;1m\u001b[1;3mFigure 1: The Transformer - model architecture.\n",
            "wise fully connected feed-forward network. We employ a residual connection [ 10] around each of\n",
            "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
            "LayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer\n",
            "itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
            "layers, produce outputs of dimension dmodel = 512 .\n",
            "Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\n",
            "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\n",
            "attention over the output of the encoder stack. Similar to the encoder, we employ residual connections\n",
            "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
            "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
            "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
            "predictions for position ican depend only on the known outputs at positions less than i.\n",
            "3.2 Attention\n",
            "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
            "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
            "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
            "query with the corresponding key.\n",
            "3.2.1 Scaled Dot-Product Attention\n",
            "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
            "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
            "3\n",
            "\n",
            "MultiHead( Q,K,V ) = Concat(head 1,...,head h)WO\n",
            "where head i= Attention( QWQ\n",
            "i,KWK\n",
            "i,VWV\n",
            "i)\n",
            "Where the projections are parameter matrices WQ\n",
            "i∈Rdmodel×dk,WK\n",
            "i∈Rdmodel×dk,WV\n",
            "i∈Rdmodel×dv\n",
            "andWO∈Rhdv×dmodel.\n",
            "In this work we employ h= 8 parallel attention layers, or heads. For each of these we use\n",
            "dk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\n",
            "is similar to that of single-head attention with full dimensionality.\n",
            "3.2.3 Applications of Attention in our Model\n",
            "The Transformer uses multi-head attention in three different ways:\n",
            "•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
            "and the memory keys and values come from the output of the encoder. This allows every\n",
            "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
            "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
            "[31, 2, 8].\n",
            "•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
            "and queries come from the same place, in this case, the output of the previous layer in the\n",
            "encoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
            "encoder.\n",
            "•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
            "all positions in the decoder up to and including that position. We need to prevent leftward\n",
            "information ﬂow in the decoder to preserve the auto-regressive property. We implement this\n",
            "inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\n",
            "of the softmax which correspond to illegal connections. See Figure 2.\n",
            "3.3 Position-wise Feed-Forward Networks\n",
            "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
            "connected feed-forward network, which is applied to each position separately and identically. This\n",
            "consists of two linear transformations with a ReLU activation in between.\n",
            "\n",
            "Scaled Dot-Product Attention\n",
            " Multi-Head Attention\n",
            "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
            "attention layers running in parallel.\n",
            "query with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\n",
            "values.\n",
            "In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
            "into a matrix Q. The keys and values are also packed together into matrices KandV. We compute\n",
            "the matrix of outputs as:\n",
            "Attention(Q,K,V ) = softmax(QKT\n",
            "√dk)V (1)\n",
            "The two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\n",
            "plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
            "of1√dk. Additive attention computes the compatibility function using a feed-forward network with\n",
            "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\n",
            "much faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\n",
            "matrix multiplication code.\n",
            "While for small values of dkthe two mechanisms perform similarly, additive attention outperforms\n",
            "dot product attention without scaling for larger values of dk[3]. We suspect that for large values of\n",
            "dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
            "extremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\n",
            "3.2.2 Multi-Head Attention\n",
            "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
            "we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\n",
            "linear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\n",
            "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
            "output values. These are concatenated and once again projected, resulting in the ﬁnal values, as\n",
            "depicted in Figure 2.\n",
            "Multi-head attention allows the model to jointly attend to information from different representation\n",
            "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
            "4To illustrate why the dot products get large, assume that the components of qandkare independent random\n",
            "\n",
            "[9], consuming the previously generated symbols as additional input when generating the next.\n",
            "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
            "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
            "respectively.\n",
            "3.1 Encoder and Decoder Stacks\n",
            "Encoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\n",
            "sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\n",
            "2\u001b[0m\u001b[32mExported 20 spans successfully.\u001b[39m\n",
            "\u001b[32;1m\u001b[1;3mThought: The retriever tool is still not providing a concise and direct answer to how attention weights are calculated. I need to be more specific and use keywords from the relevant section of the paper. \n",
            "\n",
            "Action: retriever_tool\n",
            "Action Input: 'Provide the formula for the \"Scaled Dot-Product Attention\" from the paper and explain each component.' \u001b[0m\u001b[33;1m\u001b[1;3mScaled Dot-Product Attention\n",
            " Multi-Head Attention\n",
            "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
            "attention layers running in parallel.\n",
            "query with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\n",
            "values.\n",
            "In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
            "into a matrix Q. The keys and values are also packed together into matrices KandV. We compute\n",
            "the matrix of outputs as:\n",
            "Attention(Q,K,V ) = softmax(QKT\n",
            "√dk)V (1)\n",
            "The two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\n",
            "plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
            "of1√dk. Additive attention computes the compatibility function using a feed-forward network with\n",
            "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\n",
            "much faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\n",
            "matrix multiplication code.\n",
            "While for small values of dkthe two mechanisms perform similarly, additive attention outperforms\n",
            "dot product attention without scaling for larger values of dk[3]. We suspect that for large values of\n",
            "dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
            "extremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\n",
            "3.2.2 Multi-Head Attention\n",
            "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
            "we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\n",
            "linear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\n",
            "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
            "output values. These are concatenated and once again projected, resulting in the ﬁnal values, as\n",
            "depicted in Figure 2.\n",
            "Multi-head attention allows the model to jointly attend to information from different representation\n",
            "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
            "4To illustrate why the dot products get large, assume that the components of qandkare independent random\n",
            "\n",
            "Figure 1: The Transformer - model architecture.\n",
            "wise fully connected feed-forward network. We employ a residual connection [ 10] around each of\n",
            "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
            "LayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer\n",
            "itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
            "layers, produce outputs of dimension dmodel = 512 .\n",
            "Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\n",
            "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\n",
            "attention over the output of the encoder stack. Similar to the encoder, we employ residual connections\n",
            "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
            "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
            "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
            "predictions for position ican depend only on the known outputs at positions less than i.\n",
            "3.2 Attention\n",
            "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
            "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
            "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
            "query with the corresponding key.\n",
            "3.2.1 Scaled Dot-Product Attention\n",
            "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
            "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
            "3\n",
            "\n",
            "output values. These are concatenated and once again projected, resulting in the ﬁnal values, as\n",
            "depicted in Figure 2.\n",
            "Multi-head attention allows the model to jointly attend to information from different representation\n",
            "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
            "4To illustrate why the dot products get large, assume that the components of qandkare independent random\n",
            "variables with mean 0and variance 1. Then their dot product, q·k=∑dk\n",
            "i=1qiki, has mean 0and variance dk.\n",
            "4\n",
            "\n",
            "MultiHead( Q,K,V ) = Concat(head 1,...,head h)WO\n",
            "where head i= Attention( QWQ\n",
            "i,KWK\n",
            "i,VWV\n",
            "i)\n",
            "Where the projections are parameter matrices WQ\n",
            "i∈Rdmodel×dk,WK\n",
            "i∈Rdmodel×dk,WV\n",
            "i∈Rdmodel×dv\n",
            "andWO∈Rhdv×dmodel.\n",
            "In this work we employ h= 8 parallel attention layers, or heads. For each of these we use\n",
            "dk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\n",
            "is similar to that of single-head attention with full dimensionality.\n",
            "3.2.3 Applications of Attention in our Model\n",
            "The Transformer uses multi-head attention in three different ways:\n",
            "•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
            "and the memory keys and values come from the output of the encoder. This allows every\n",
            "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
            "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
            "[31, 2, 8].\n",
            "•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
            "and queries come from the same place, in this case, the output of the previous layer in the\n",
            "encoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
            "encoder.\n",
            "•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
            "all positions in the decoder up to and including that position. We need to prevent leftward\n",
            "information ﬂow in the decoder to preserve the auto-regressive property. We implement this\n",
            "inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\n",
            "of the softmax which correspond to illegal connections. See Figure 2.\n",
            "3.3 Position-wise Feed-Forward Networks\n",
            "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
            "connected feed-forward network, which is applied to each position separately and identically. This\n",
            "consists of two linear transformations with a ReLU activation in between.\u001b[0m\u001b[32mExported 20 spans successfully.\u001b[39m\n",
            "\u001b[32;1m\u001b[1;3mThought: The retriever tool finally provided the formula for \"Scaled Dot-Product Attention\". Now I can break down the formula and explain how the attention weights are calculated. \n",
            "\n",
            "Final Answer: \n",
            "\n",
            "The attention weights in \"Scaled Dot-Product Attention\" are calculated using the following formula:\n",
            "\n",
            "```\n",
            "Attention(Q, K, V) = softmax(Q*K^T / sqrt(d_k)) * V\n",
            "```\n",
            "\n",
            "Here's a breakdown of the formula:\n",
            "\n",
            "* **Q (Queries):**  Represents the matrix of query vectors. Each query vector represents a specific word or position in the input sequence.\n",
            "* **K (Keys):** Represents the matrix of key vectors. Each key vector is associated with a specific word or position in the input sequence.\n",
            "* **V (Values):** Represents the matrix of value vectors. Each value vector contains information about a specific word or position in the input sequence.\n",
            "* **d_k:** Represents the dimension of the key vectors.\n",
            "\n",
            "**Calculation Steps:**\n",
            "\n",
            "1. **Dot Product (Q * K^T):**  The dot product of each query vector with all key vectors is calculated. This measures the similarity between each query and key.\n",
            "2. **Scaling (/ sqrt(d_k)):**  The dot products are scaled down by dividing by the square root of the key vector dimension (d_k). This prevents the dot products from becoming too large, which can lead to unstable training.\n",
            "3. **Softmax:** The scaled dot products are passed through a softmax function. This normalizes the values into a probability distribution, where each value represents the attention weight assigned to the corresponding value vector.\n",
            "4. **Weighted Sum (* V):** Finally, a weighted sum of the value vectors is calculated, using the attention weights obtained from the softmax. This means that words or positions with higher attention weights will contribute more to the final output.\n",
            "\n",
            "In essence, \"Scaled Dot-Product Attention\" calculates attention weights by measuring the similarity between query and key vectors, normalizing these similarities into probabilities, and then using these probabilities to create a weighted average of the value vectors. This allows the model to focus on the most relevant parts of the input when making predictions. \n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Can you explain the concept of Attention in transformers?',\n",
              " 'output': 'The attention weights in \"Scaled Dot-Product Attention\" are calculated using the following formula:\\n\\n```\\nAttention(Q, K, V) = softmax(Q*K^T / sqrt(d_k)) * V\\n```\\n\\nHere\\'s a breakdown of the formula:\\n\\n* **Q (Queries):**  Represents the matrix of query vectors. Each query vector represents a specific word or position in the input sequence.\\n* **K (Keys):** Represents the matrix of key vectors. Each key vector is associated with a specific word or position in the input sequence.\\n* **V (Values):** Represents the matrix of value vectors. Each value vector contains information about a specific word or position in the input sequence.\\n* **d_k:** Represents the dimension of the key vectors.\\n\\n**Calculation Steps:**\\n\\n1. **Dot Product (Q * K^T):**  The dot product of each query vector with all key vectors is calculated. This measures the similarity between each query and key.\\n2. **Scaling (/ sqrt(d_k)):**  The dot products are scaled down by dividing by the square root of the key vector dimension (d_k). This prevents the dot products from becoming too large, which can lead to unstable training.\\n3. **Softmax:** The scaled dot products are passed through a softmax function. This normalizes the values into a probability distribution, where each value represents the attention weight assigned to the corresponding value vector.\\n4. **Weighted Sum (* V):** Finally, a weighted sum of the value vectors is calculated, using the attention weights obtained from the softmax. This means that words or positions with higher attention weights will contribute more to the final output.\\n\\nIn essence, \"Scaled Dot-Product Attention\" calculates attention weights by measuring the similarity between query and key vectors, normalizing these similarities into probabilities, and then using these probabilities to create a weighted average of the value vectors. This allows the model to focus on the most relevant parts of the input when making predictions.'}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5wT3RzHf_D9M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}